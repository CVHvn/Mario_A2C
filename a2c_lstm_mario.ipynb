{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19939a9c-03f0-4150-9477-9109e9735319",
      "metadata": {
        "id": "19939a9c-03f0-4150-9477-9109e9735319"
      },
      "source": [
        "# install packages\n",
        "install packages need to train mario agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d",
        "outputId": "2a5da188-df12-4bf5-a301-5804fc9d068d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.52.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nes-py>=8.1.4 (from gym-super-mario-bros)\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.25.2)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros)\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.66.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.0.8)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535717 sha256=c4a64388a13d2628d1c96581067805a9c607c00580c59f435d8e308c60adf154\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n",
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.4.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (67.7.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.25.2)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (4.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install gym-super-mario-bros==7.4.0\n",
        "!pip install gym==0.25.2\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install imageio\n",
        "!pip install torchvision\n",
        "!pip install opencv-python-headless"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829519d4-5bf4-46ed-8b52-8f03fee2f8d9",
      "metadata": {
        "id": "829519d4-5bf4-46ed-8b52-8f03fee2f8d9"
      },
      "source": [
        "# import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f",
      "metadata": {
        "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import copy\n",
        "import cv2\n",
        "import imageio\n",
        "import numpy as np\n",
        "import random, os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "#import multiprocessing as mp\n",
        "from torchvision import transforms as T\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd03a2e1-8ccd-4dd5-a13d-87f265c1c55e",
      "metadata": {
        "id": "bd03a2e1-8ccd-4dd5-a13d-87f265c1c55e"
      },
      "source": [
        "# Create hyperparammeters\n",
        "Config hyperparammeters, just change it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a95999c-5111-4054-9256-45cd206a697e",
      "metadata": {
        "id": "4a95999c-5111-4054-9256-45cd206a697e"
      },
      "outputs": [],
      "source": [
        "#class DictWrapper created by Chatgpt\n",
        "class DictWrapper:\n",
        "    def __init__(self, dictionary):\n",
        "        self._dict = dictionary\n",
        "\n",
        "    def __getattr__(self, item):\n",
        "        if item in self._dict:\n",
        "            return self._dict[item]\n",
        "        else:\n",
        "            raise AttributeError(f\"'DictWrapper' object has no attribute '{item}'\")\n",
        "\n",
        "config = {\n",
        "    'num_envs': 16,\n",
        "    'save_model_step': int(1e5),\n",
        "    'save_figure_step': int(1e3),\n",
        "    'learn_step': 20,\n",
        "    'total_step_or_episode': 'step',\n",
        "    'total_step': int(5e6),\n",
        "    'total_episode': None,\n",
        "    'save_dir': \"\",\n",
        "    'gamma': 0.9,\n",
        "    'learning_rate': 1e-4,\n",
        "    'state_dim': (1, 84, 84),\n",
        "    'action_dim': 12,#12 for complex, 7 for simple\n",
        "    'entropy_coef': 0.01,\n",
        "    'V_coef': 0.5,\n",
        "    'max_grad_norm': 0.5,\n",
        "    'world': 1,\n",
        "    'stage': 1,\n",
        "    'action_type': 'complex'\n",
        "}\n",
        "\n",
        "config = DictWrapper(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ha80HrVm1_OR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ha80HrVm1_OR",
        "outputId": "05c74b3e-1ae5-4b63-b38d-7fd2089a7ba2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.25.2'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gym.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "440a3f9c-d1fd-41e5-a431-592e719aef84",
      "metadata": {
        "id": "440a3f9c-d1fd-41e5-a431-592e719aef84"
      },
      "source": [
        "# Define environment\n",
        "## Create a custom environment, We need:\n",
        "- SkipFrame: because the episode is very long then in this environment, we only need to repeat action sometimes. Then we repeat each action 4 times and skip 3 first frames (return 4th frame). We need to sum 4 rewards from 4 frames.\n",
        "- GrayScaleResizeObservation: grayscale state (from RGB to gray image) and resize to 84x84.\n",
        "- NoopResetEnv: when resetting the environment, I do random actions sometimes before using the evironment. This same as atari strategy. When reset, we choice num_noops random actions by random from 0 to noop_max and random do num_noops actions. If random actions yield to a terminal state, then reset and continue do random action. I set noop_max as 30 same as atari.\n",
        "- CustomRewardAndDoneEnv: I see that a lot of people train Mario to use this custom reward, and then I copy it. Just add 50 rewards if the agent solves the stage and add -50 rewards if the agent is dead. And the reward is divided into 10. I set done to True if Mario dies instead of when Mario lost all lives as default.\n",
        "- stage 4-2: give a -50 reward when Mario moves on top of the map (y_pos >= 255).\n",
        "- with stages 4-4 and 7-4: set done = True when Mario goes to wrong way and gives a -50 penalty reward. If Mario goes to the true way but the map still loops (this is a bug), I set done = True but don't give a penalty reward.\n",
        "\n",
        "## About reward system:\n",
        "- set done to True when Mario dies: This is the most important thing because, in the default reward system, Mario still yields a reward by just moving right, if Mario dies, the agent can't lose total rewards and the agent still go right (in new life) and get more rewards. This is the easiest way to get rewards and the agent also learns this trick.\n",
        "- penalty -50 reward when Mario dies: this is necessary to make Mario train faster. If we don't add a penalty, the agent can't complete hard stages.\n",
        "reward 50 when reaching the flag: this makes Mario train faster and surpass stuck points in hard stages.\n",
        "- change penalty and reaching flag reward of more or less than 50 don't make more difference then I don't change it.\n",
        "- Divide rewards by 10: I think it makes total rewards smaller and the agent can learn a better way, but I am not sure how it is necessary. I just copy it.\n",
        "- With Stage 4-2: I see that the agent can reach more rewards when Mario goes to the wrap zone, but Mario can't win this stage with the wrap zone way because the reward system gives negative rewards when Mario goes left. Then I just give a penalty reward when Mario moves to the top of the map.\n",
        "- with stage 4-4 and 7-4:\n",
        "+ because this map has the wrong way, Mario will go to the loop and the reward still increase to infinity, then I need to set done = True and give a negative penalty reward\n",
        "+ negative reward to prevent Mario go to the wrong way.\n",
        "+ Another strategy is to give a negative reward but don't set done = True (like 4-2). But this map has a bug and this strangtegy not working.\n",
        "+ even though Mario goes in the correct way, sometimes Mario still goes in the loop. Then I set done = True every time Mario goes on the loop (check by x_pos and max_x_pos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da",
        "outputId": "1199a203-1dad-48a7-b1f9-e8c9a61d9cc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(12)\n",
            "(240, 256, 3),\n",
            " 0.0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(f\"SuperMarioBros-{config.world}-{config.stage}-v0\", new_step_api=True)\n",
        "else:\n",
        "    env = gym_super_mario_bros.make(f\"SuperMarioBros-{config.world}-{config.stage}-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
        "print(env.action_space)\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, trunk, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunk, info\n",
        "\n",
        "class GrayScaleResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.current_state = observation\n",
        "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
        "        observation = observation.astype(np.uint8).reshape(-1, observation.shape[0], observation.shape[1])\n",
        "        return observation\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        obs = self.env.reset(**kwargs)\n",
        "        noops = np.random.randint(0, self.noop_max, (1, ))[0]\n",
        "        for _ in range(noops):\n",
        "            action = self.env.action_space.sample()\n",
        "            obs, _, done, _, _ = self.env.step(action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        obs, reward, done, trunk, info = self.env.step(ac)\n",
        "        return obs, reward, done, trunk, info\n",
        "\n",
        "class CustomRewardAndDoneEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, world=1, stage=1):\n",
        "        super(CustomRewardAndDoneEnv, self).__init__(env)\n",
        "        self.current_score = 0\n",
        "        self.current_x = 0\n",
        "        self.current_x_count = 0\n",
        "        self.max_x = 0\n",
        "        self.world = world\n",
        "        self.stage = stage\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.current_score = 0\n",
        "        self.current_x = 0\n",
        "        self.current_x_count = 0\n",
        "        self.max_x = 0\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, trunc, info = self.env.step(action)\n",
        "        if (info['x_pos'] - self.current_x) == 0:\n",
        "            self.current_x_count += 1\n",
        "        else:\n",
        "            self.current_x_count = 0\n",
        "        if info[\"flag_get\"]:\n",
        "            reward += 50\n",
        "            done = True\n",
        "        if done and info[\"flag_get\"] == False and info[\"time\"] != 0:\n",
        "            reward -= 50\n",
        "            done = True\n",
        "        self.current_score = info[\"score\"]\n",
        "        self.current_x = info[\"x_pos\"]\n",
        "        self.max_x = max(self.max_x, self.current_x)\n",
        "\n",
        "        if self.world == 7 and self.stage == 4:\n",
        "            if (506 <= info[\"x_pos\"] <= 832 and info[\"y_pos\"] > 127) or (\n",
        "                    832 < info[\"x_pos\"] <= 1064 and info[\"y_pos\"] < 80) or (\n",
        "                    1113 < info[\"x_pos\"] <= 1464 and info[\"y_pos\"] < 191) or (\n",
        "                    1579 < info[\"x_pos\"] <= 1943 and info[\"y_pos\"] < 191) or (\n",
        "                    1946 < info[\"x_pos\"] <= 1964 and info[\"y_pos\"] >= 191) or (\n",
        "                    1984 < info[\"x_pos\"] <= 2060 and (info[\"y_pos\"] >= 191 or info[\"y_pos\"] < 127)) or (\n",
        "                    2114 < info[\"x_pos\"] < 2440 and info[\"y_pos\"] < 191):\n",
        "                reward -= 50\n",
        "                done = True\n",
        "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
        "                done = True\n",
        "        if self.world == 4 and self.stage == 4:\n",
        "            if (info[\"x_pos\"] <= 1500 and info[\"y_pos\"] < 127) or (\n",
        "                    1588 <= info[\"x_pos\"] < 2380 and info[\"y_pos\"] >= 127):\n",
        "                reward = -50\n",
        "                done = True\n",
        "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
        "                done = True\n",
        "            if done == False:\n",
        "                reward -= 0.1\n",
        "        if self.world == 4 and self.stage == 2 and done == False and info['y_pos'] >= 255:\n",
        "            reward -= 50\n",
        "\n",
        "        return state, reward / 10., done, trunc, info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec30b8e-1b8d-487f-98f3-21615ce704ee",
      "metadata": {
        "id": "6ec30b8e-1b8d-487f-98f3-21615ce704ee"
      },
      "source": [
        "# Create MultipleEnvironments\n",
        "MultipleEnvironments use multi-processing to parallel running.\n",
        "\n",
        "Because in the training process, we need to reset the environment when the agent reaches the terminal state. But if we will do it in parallel, then I don't want to check each environment and reset (by loop) or create a new function that parallels check and reset all environments. Then I reset the environment if done = True in step function and set next_state = env.reset(). Then in training, we just set state = next_state (next_state is reset state if done = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y2mcbU_JvQwI",
      "metadata": {
        "id": "y2mcbU_JvQwI"
      },
      "outputs": [],
      "source": [
        "#modify from https://github.com/uvipen/Super-mario-bros-PPO-pytorch/blob/master/src/env.py\n",
        "def create_env(world, stage, action_type, test=False):\n",
        "    if gym.__version__ < '0.26':\n",
        "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", new_step_api=True)\n",
        "    else:\n",
        "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "    if action_type == \"right\":\n",
        "        action_type = RIGHT_ONLY\n",
        "    elif action_type == \"simple\":\n",
        "        action_type = SIMPLE_MOVEMENT\n",
        "    else:\n",
        "        action_type = COMPLEX_MOVEMENT\n",
        "\n",
        "    env = JoypadSpace(env, action_type)\n",
        "\n",
        "    if test == False:\n",
        "        env = NoopResetEnv(env)\n",
        "    env = SkipFrame(env, skip=4)\n",
        "    env = CustomRewardAndDoneEnv(env, world, stage)\n",
        "    env = GrayScaleResizeObservation(env, shape=84)\n",
        "    # if gym.__version__ < '0.26':\n",
        "    #     env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "    # else:\n",
        "    #     env = FrameStack(env, num_stack=4)\n",
        "    return env\n",
        "\n",
        "class MultipleEnvironments:\n",
        "    def __init__(self, world, stage, action_type, num_envs):\n",
        "        self.agent_conns, self.env_conns = zip(*[mp.Pipe(duplex=True) for _ in range(num_envs)])\n",
        "        self.envs = [create_env(world, stage, action_type) for _ in range(num_envs)]\n",
        "\n",
        "        for index in range(num_envs):\n",
        "            process = mp.Process(target=self.run, args=(index,))\n",
        "            process.start()\n",
        "            self.env_conns[index].close()\n",
        "\n",
        "    def run(self, index):\n",
        "        self.agent_conns[index].close()\n",
        "        while True:\n",
        "            request, action = self.env_conns[index].recv()\n",
        "            if request == \"step\":\n",
        "                next_state, reward, done, trunc, info = self.envs[index].step(action)\n",
        "                if done:\n",
        "                    next_state = self.envs[index].reset()\n",
        "                self.env_conns[index].send((next_state, reward, done, trunc, info))\n",
        "            elif request == \"reset\":\n",
        "                self.env_conns[index].send(self.envs[index].reset())\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "    def step(self, actions):\n",
        "        [agent_conn.send((\"step\", act)) for agent_conn, act in zip(self.agent_conns, actions)]\n",
        "        next_states, rewards, dones, truncs, infos = zip(*[agent_conn.recv() for agent_conn in self.agent_conns])\n",
        "        return next_states, rewards, dones, truncs, infos\n",
        "\n",
        "    def reset(self):\n",
        "        [agent_conn.send((\"reset\", None)) for agent_conn in self.agent_conns]\n",
        "        states = [agent_conn.recv() for agent_conn in self.agent_conns]\n",
        "        return states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31dycPWmOuOR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31dycPWmOuOR",
        "outputId": "ebfb5c73-f02d-4bf3-f2f3-026ee179b9b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mp.cpu_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a894f8-f303-4fac-b478-0b191f38f274",
      "metadata": {
        "id": "92a894f8-f303-4fac-b478-0b191f38f274"
      },
      "source": [
        "# Create memory\n",
        "Memory just save all info we need to train and return all stored info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1736a4f8-d863-4bed-b290-3812f6c43eae",
      "metadata": {
        "id": "1736a4f8-d863-4bed-b290-3812f6c43eae"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "    def __init__(self, num_envs):\n",
        "        self.num_envs = num_envs\n",
        "\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.next_states = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logits = []\n",
        "        self.values = []\n",
        "\n",
        "    def save(self, state, action, reward, next_state, done, logit, V):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.next_states.append(next_state)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.logits.append(logit)\n",
        "        self.values.append(V)\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.next_states = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logits = []\n",
        "        self.values = []\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.states, self.actions, self.next_states, self.rewards, self.dones, self.logits, self.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4303a12-57dc-46c5-9134-5d2e20e0438f",
      "metadata": {
        "id": "c4303a12-57dc-46c5-9134-5d2e20e0438f"
      },
      "source": [
        "# Create agent\n",
        "The agent includes 4 main functions:\n",
        "## train\n",
        "train function train agent via many episodes:\n",
        "- first, we create (h, c) as zeros. Reset the first state.\n",
        "- loop until the agent wins this stage or reaches the maximum episode/step:\n",
        "- predict value, logit, h, c for the current state\n",
        "- sample action from logit with category distribution (select_action function)\n",
        "- log all info to memory\n",
        "- train agent every learn_step (learn function)\n",
        "- eval agent every save_figure_step (save_figure function)\n",
        "- reset (h, c) to zeros if the agent reaches the terminal state\n",
        "- set state = next_state (I reset environment when agent reach terminal state then next_state is first state if done=True)\n",
        "\n",
        "## select_action\n",
        "this function sample action from logit:\n",
        "- just convert logit to probability: policy = F.softmax(logits, dim=1)\n",
        "- create distribution from probability: distribution = torch.distributions.Categorical(policy)\n",
        "- sample action from distribution: actions = distribution.sample()\n",
        "\n",
        "## save_figure\n",
        "this function eval agent and saves agent/video if the agent yields better total rewards:\n",
        "- create (h, c) as zeros, and reset the environment.\n",
        "- loop until the agent reaches the terminal state.\n",
        "- predict logit from model\n",
        "- get action = argmax (logit)\n",
        "- environment do this action to get next_state, reward, info, done\n",
        "- if total_reward > best test total reward or agent complete this stage, I save model and video.\n",
        "- if agent completes this state, we stop training.\n",
        "\n",
        "## learn\n",
        "this function trains agent from the experiment saved in memory\n",
        "- get all the info from memory\n",
        "- calculate td (lambda) target and gae advantages\n",
        "- calculate loss\n",
        "- norm gradient\n",
        "- update model from loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d-yL01WMumqF",
      "metadata": {
        "id": "d-yL01WMumqF"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, world, stage, action_type, envs, num_envs, state_dim, action_dim, save_dir, save_model_step,\n",
        "                 save_figure_step, learn_step, total_step_or_episode, total_step, total_episode, model,\n",
        "                 gamma, learning_rate, entropy_coef, V_coef, max_grad_norm, device):\n",
        "        self.world = world\n",
        "        self.stage = stage\n",
        "        self.action_type = action_type\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "        self.learn_step = learn_step\n",
        "        self.total_step_or_episode = total_step_or_episode\n",
        "        self.total_step = total_step\n",
        "        self.total_episode = total_episode\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.current_episode = 0\n",
        "\n",
        "        self.save_model_step = save_model_step\n",
        "        self.save_figure_step = save_figure_step\n",
        "\n",
        "        self.device = device\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.num_envs = num_envs\n",
        "        self.envs = envs\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.V_coef = V_coef\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        self.memory = Memory(self.num_envs)\n",
        "\n",
        "        self.is_completed = False\n",
        "\n",
        "        self.env = None\n",
        "        self.max_test_score = -1e9\n",
        "\n",
        "        # I just log 1000 lastest update and print it to log.\n",
        "        self.V_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.P_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.E_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.total_loss = np.zeros((1000,)).reshape(-1)\n",
        "        self.loss_index = 0\n",
        "        self.len_loss = 0\n",
        "\n",
        "    def save_figure(self, is_training=False):\n",
        "        # test current model and save model/figure if model yield best total rewards.\n",
        "        # create env for testing, reset test env\n",
        "        if self.env is None:\n",
        "            self.env = create_env(self.world, self.stage, self.action_type, True)\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "\n",
        "        images = []\n",
        "        total_reward = 0\n",
        "        total_step = 0\n",
        "        num_repeat_action = 0\n",
        "        old_action = -1\n",
        "\n",
        "        # create h, c as zeros\n",
        "        h = torch.zeros((1, 512), dtype=torch.float, device = self.device)\n",
        "        c = torch.zeros((1, 512), dtype=torch.float, device = self.device)\n",
        "\n",
        "        episode_time = datetime.now()\n",
        "\n",
        "        # play 1 episode, just get loop action with max probability from model until the episode end.\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                logit, V, h, c = self.model(torch.tensor(state, dtype = torch.float, device = self.device).unsqueeze(0), h, c)\n",
        "            action = logit.argmax(-1).item()\n",
        "            next_state, reward, done, trunc, info = self.env.step(action)\n",
        "            state = next_state\n",
        "            img = Image.fromarray(self.env.current_state)\n",
        "            images.append(img)\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "\n",
        "            if action == old_action:\n",
        "                num_repeat_action += 1\n",
        "            else:\n",
        "                num_repeat_action = 0\n",
        "            old_action = action\n",
        "            if num_repeat_action == 200:\n",
        "                break\n",
        "\n",
        "        #logging, if model yield better result, save figure (test_episode.mp4) and model (best_model.pth)\n",
        "        if is_training:\n",
        "            f_out = open(f\"logging_test.txt\", \"a\")\n",
        "            f_out.write(f'episode_reward: {total_reward} episode_step: {total_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} episode_time: {datetime.now() - episode_time}\\n')\n",
        "            f_out.close()\n",
        "\n",
        "        if total_reward > self.max_test_score or info['flag_get']:\n",
        "            imageio.mimsave('test_episode.mp4', images)\n",
        "            self.max_test_score = total_reward\n",
        "            if is_training:\n",
        "                torch.save(self.model.state_dict(), f\"best_model.pth\")\n",
        "\n",
        "        # if model can complete this game, stop training by set self.is_completed to True\n",
        "        if info['flag_get']:\n",
        "            self.is_completed = True\n",
        "\n",
        "    def save_model(self):\n",
        "        torch.save(self.model.state_dict(), f\"model_{self.current_step}.pth\")\n",
        "\n",
        "    def load_model(self, model_path = None):\n",
        "        if model_path is None:\n",
        "            model_path = f\"model_{self.current_step}.pth\"\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    def select_action(self, states, h, c):\n",
        "        # select action when training, we need use Categorical distribution to make action base on probability from model\n",
        "        states = torch.tensor(np.array(states), device = self.device)\n",
        "\n",
        "        logits, V, h, c = self.model(states, h, c)\n",
        "        with torch.no_grad():\n",
        "            policy = F.softmax(logits, dim=1)\n",
        "            distribution = torch.distributions.Categorical(policy)\n",
        "            actions = distribution.sample().cpu().numpy().tolist()\n",
        "        return actions, logits, V, h, c\n",
        "\n",
        "    def update_loss_statis(self, loss_p, loss_v, loss_e, loss):\n",
        "        # update loss for logging, just save 1000 latest updates.\n",
        "        self.V_loss[self.loss_index] = loss_v\n",
        "        self.P_loss[self.loss_index] = loss_p\n",
        "        self.E_loss[self.loss_index] = loss_e\n",
        "        self.total_loss[self.loss_index] = loss\n",
        "        self.loss_index = (self.loss_index + 1)%1000\n",
        "        self.len_loss = min(self.len_loss+1, 1000)\n",
        "\n",
        "    def learn(self, h, c):\n",
        "        # reset optimizer\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # get all data\n",
        "        states, actions, next_states, rewards, dones, logits, values = self.memory.get_data()\n",
        "\n",
        "        # calculate target (td lambda target) and gae advantages\n",
        "        targets = []\n",
        "        with torch.no_grad():\n",
        "            _, next_value, h, c = self.model(torch.tensor(np.array(next_states[-1]), device = self.device), h, c)\n",
        "        target = next_value\n",
        "        advantage = 0\n",
        "\n",
        "        for state, next_state, reward, done, V in zip(states[::-1], next_states[::-1], rewards[::-1], dones[::-1], values[::-1]):\n",
        "            done = torch.tensor(done, device = self.device, dtype = torch.float).reshape(-1, 1)\n",
        "            reward = torch.tensor(reward, device = self.device).reshape(-1, 1)\n",
        "\n",
        "            target = next_value * self.gamma * (1-done) + reward\n",
        "            advantage = target + self.gamma * advantage * (1-done)\n",
        "            targets.append(advantage)\n",
        "            advantage = advantage - V.detach()\n",
        "            next_value = V.detach()\n",
        "        targets = targets[::-1]\n",
        "\n",
        "        # convert all data to tensor\n",
        "        values = torch.cat(values, 0)\n",
        "        targets = torch.cat(targets, 0).view(-1, 1)\n",
        "        logits = torch.cat(logits, 0)\n",
        "        probs = torch.softmax(logits, -1)\n",
        "        advantages = (targets - values).reshape(-1)\n",
        "\n",
        "        # calculate loss\n",
        "        entropy = (- (probs * (probs + 1e-9).log()).sum(-1)).mean()\n",
        "        loss_V = F.smooth_l1_loss(values, targets)\n",
        "\n",
        "        index = torch.arange(0, len(probs), device = self.device)\n",
        "        actions = torch.flatten(torch.tensor(actions, device = self.device, dtype = torch.int64))\n",
        "        loss_P = -((probs[index, actions] + 1e-9).log() * advantages.detach()).mean()\n",
        "\n",
        "        loss = - entropy * self.entropy_coef + loss_V * self.V_coef + loss_P\n",
        "        loss.backward()\n",
        "\n",
        "        # norm gradient and update agent\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.update_loss_statis(loss_P.item(), loss_V.item(), entropy.item(), loss.item())\n",
        "\n",
        "    def train(self):\n",
        "        episode_reward = [0] * self.num_envs\n",
        "        episode_step = [0] * self.num_envs\n",
        "        max_episode_reward = 0\n",
        "        max_episode_step = 0\n",
        "        episode_time = [datetime.now() for _ in range(self.num_envs)]\n",
        "        total_time = datetime.now()\n",
        "\n",
        "        last_episode_rewards = []\n",
        "\n",
        "        # reset envs\n",
        "        states = self.envs.reset()\n",
        "\n",
        "        # create h, c as zeros\n",
        "        h = torch.zeros((self.num_envs, 512), dtype=torch.float, device = self.device)\n",
        "        c = torch.zeros((self.num_envs, 512), dtype=torch.float, device = self.device)\n",
        "\n",
        "        while True:\n",
        "            # finish training if agent reach total_step or total_episode based on what type of total_step_or_episode is step or episode\n",
        "            self.current_step += 1\n",
        "\n",
        "            if self.total_step_or_episode == 'step':\n",
        "                if self.current_step >= self.total_step:\n",
        "                    break\n",
        "            else:\n",
        "                if self.current_episode >= self.total_episode:\n",
        "                    break\n",
        "\n",
        "            actions, logit, V, h, c = self.select_action(states, h, c)\n",
        "\n",
        "            next_states, rewards, dones, truncs, infos = self.envs.step(actions)\n",
        "\n",
        "            # save to memory\n",
        "            self.memory.save(states, actions, rewards, next_states, dones, logit, V)\n",
        "\n",
        "            episode_reward = [x + reward for x, reward in zip(episode_reward, rewards)]\n",
        "            episode_step = [x+1 for x in episode_step]\n",
        "\n",
        "            # logging after each step, if 1 episode is ending, I will log this to logging.txt\n",
        "            for i, done in enumerate(dones):\n",
        "                if done:\n",
        "                    self.current_episode += 1\n",
        "                    max_episode_reward = max(max_episode_reward, episode_reward[i])\n",
        "                    max_episode_step = max(max_episode_step, episode_step[i])\n",
        "                    last_episode_rewards.append(episode_reward[i])\n",
        "                    f_out = open(f\"logging.txt\", \"a\")\n",
        "                    f_out.write(f'episode: {self.current_episode} agent: {i} rewards: {episode_reward[i]:.4f} steps: {episode_step[i]} complete: {infos[i][\"flag_get\"]==True} mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean():.4f} max_rewards: {max_episode_reward:.4f} max_steps: {max_episode_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} episode_time: {datetime.now() - episode_time[i]} total_time: {datetime.now() - total_time}\\n')\n",
        "                    f_out.close()\n",
        "                    episode_reward[i] = 0\n",
        "                    episode_step[i] = 0\n",
        "                    episode_time[i] = datetime.now()\n",
        "\n",
        "            # reset h and c to zeros for enviroments that just ending episode, just multiply h and c with (1-dones)\n",
        "            h = h * (1 - torch.tensor(dones, device = self.device, dtype = torch.float).reshape(-1, 1))\n",
        "            c = c * (1 - torch.tensor(dones, device = self.device, dtype = torch.float).reshape(-1, 1))\n",
        "\n",
        "            # training agent every learn_step\n",
        "            if self.current_step % self.learn_step == 0:\n",
        "                self.learn(h, c)\n",
        "                self.memory.reset()\n",
        "\n",
        "            if self.current_step % self.save_model_step == 0:\n",
        "                self.save_model()\n",
        "\n",
        "            # eval agent every save_figure_step\n",
        "            if self.current_step % self.save_figure_step == 0 and self.save_figure_step != -1:\n",
        "                self.save_figure(is_training=True)\n",
        "                if self.is_completed:\n",
        "                    f_out = open(f\"logging.txt\", \"a\")\n",
        "                    f_out.write(f' mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean()} max_rewards: {max_episode_reward} max_steps: {max_episode_step} current_step: {self.current_step} total_time: {datetime.now() - total_time}\\n')\n",
        "                    f_out.close()\n",
        "                    return\n",
        "\n",
        "            states = list(next_states)\n",
        "\n",
        "        f_out = open(f\"logging.txt\", \"a\")\n",
        "        f_out.write(f' mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean()} max_rewards: {max_episode_reward} max_steps: {max_episode_step} current_step: {self.current_step} total_time: {datetime.now() - total_time}\\n')\n",
        "        f_out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718b77f8-186a-4f82-b7e9-330d7ac8098e",
      "metadata": {
        "id": "718b77f8-186a-4f82-b7e9-330d7ac8098e"
      },
      "source": [
        "# Create model\n",
        "I use the same model architecture as [github/uvipen/Super-mario-bros-A3C-pytorch/model](https://github.com/uvipen/Super-mario-bros-A3C-pytorch/blob/master/src/model.py).\n",
        "Model includes:\n",
        "- 4 convolution layers to encode input image (observation) to feature vector.\n",
        "- LSTM Cell to capture past observations.\n",
        "- two linear layers for policy and value prediction (actor and critic)\n",
        "\n",
        "To understand how LSTM (recurrent network) works in RL, we can view (h, c) from the lastest step as input of the model. Then the model takes 3 inputs: (state, h, c) instead of just state as a non-recurrent model. Then we just save output (h, c) for each state and reuse it as input with (next_state, h, c).\n",
        "\n",
        "After completing this project, I just tried to remove _initialize_weights and performance did not change in some stages, then I thought _initialize_weights is not necessary, but I don't want to remove it because I am not sure this code works for all stages without _initialize_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb",
      "metadata": {
        "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_dim[0], 32, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
        "        self.lstm = nn.LSTMCell(1152, 512)\n",
        "        self.critic_linear = nn.Linear(512, 1)\n",
        "        self.actor_linear = nn.Linear(512, output_dim)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                # nn.init.kaiming_uniform_(module.weight)\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.LSTMCell):\n",
        "                nn.init.constant_(module.bias_ih, 0)\n",
        "                nn.init.constant_(module.bias_hh, 0)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        x = F.relu(self.conv1(x/255.))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        h, c = self.lstm(x, (h, c))\n",
        "        return self.actor_linear(h), self.critic_linear(h), h.detach(), c.detach()\n",
        "\n",
        "model = Model(config.state_dim, config.action_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a7606a-b416-4c17-a3c9-85a6ce07b508",
      "metadata": {
        "id": "c2a7606a-b416-4c17-a3c9-85a6ce07b508"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077a241d-419a-4f72-9f2c-7356d7858eae",
      "metadata": {
        "id": "077a241d-419a-4f72-9f2c-7356d7858eae"
      },
      "outputs": [],
      "source": [
        "envs = MultipleEnvironments(config.world, config.stage, config.action_type, config.num_envs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4637f85-679c-4034-befe-14d839485d47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4637f85-679c-4034-befe-14d839485d47",
        "outputId": "f7cea27e-9d8a-43b7-9843-8d7ccebe9ab7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<GrayScaleResizeObservation<CustomRewardAndDoneEnv<SkipFrame<NoopResetEnv<JoypadSpace<TimeLimit<OrderEnforcing<StepAPICompatibility<PassiveEnvChecker<SuperMarioBrosEnv<SuperMarioBros-1-1-v0>>>>>>>>>>>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "envs.envs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539",
      "metadata": {
        "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539"
      },
      "outputs": [],
      "source": [
        "agent = Agent(world = config.world, stage = config.stage, action_type = config.action_type, envs = envs, num_envs = config.num_envs, \n",
        "              state_dim = config.state_dim, action_dim = config.action_dim, save_dir = config.save_dir,\n",
        "              save_model_step = config.save_model_step, save_figure_step = config.save_figure_step, learn_step = config.learn_step,\n",
        "              total_step_or_episode = config.total_step_or_episode, total_step = config.total_step, total_episode = config.total_episode,\n",
        "              model = model, gamma = config.gamma, learning_rate = config.learning_rate, entropy_coef = config.entropy_coef, V_coef = config.V_coef,\n",
        "              max_grad_norm = config.max_grad_norm,\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca",
      "metadata": {
        "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca"
      },
      "outputs": [],
      "source": [
        "agent.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NLjlwqf7qdZK",
      "metadata": {
        "id": "NLjlwqf7qdZK"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4hrG4FCepxby",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hrG4FCepxby",
        "outputId": "2a27355a-65f5-49cb-a218-5de23e15e58c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "agent.load_model(\"best_model.pth\")\n",
        "agent.save_figure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EvVY0vqQqFJB",
      "metadata": {
        "id": "EvVY0vqQqFJB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
